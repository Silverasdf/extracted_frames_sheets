{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCVBwPgtXItT"
      },
      "outputs": [],
      "source": [
        "# Test with gemini in future? -- Contacted OIT about administator access. Should I sign in with a personal account to avoid this, or would this give me an issue with reimbursement\n",
        "\n",
        "# Unfortunately, I cannot use Gemini with school account, so I used my personal account instead for it\n",
        "\n",
        "# Also, I wonder if it's better to just ask the API if an image in a video is clear or not\n",
        "\n",
        "# I've noticed that I can only give the API a few images, or the token limit is broken. Furthermore, it looks like the context is kept between prompts, given what I have seen from the completed trials.\n",
        "\n",
        "# I have run out of quota for some reason for the OpenAI API -- I think it may be fine tomorrow\n",
        "\n",
        "# I have gotten Gemini to work with the video -- however, it is gatekeeping what it thinks, as it doesn't like accidents. Note that I can just insert the video into the API and it works\n",
        "\n",
        "# Instead of a range, create an array of numbers. Figure out which of these videos are clear, and then just add them to the list and only run those\n",
        "\n",
        "# Others:\n",
        "# LLaVa 1.5 -- https://colab.research.google.com/github/camenduru/LLaVA-colab/blob/main/LLaVA_13b_4bit_colab.ipynb -- I got it working with one image, but there was an error due to high traffic. Works on their own hosted page\n",
        "# Qwen-VL -- https://colab.research.google.com/github/camenduru/Qwen-VL-Chat-colab/blob/main/Qwen_VL_Chat_colab.ipynb -- Issue with particular colab run\n",
        "# CogVLM (Probably not compatible with Colab -- requires further research)\n",
        "# BakLLaVa\n",
        "\n",
        "#I'm now getting issues for the T4 connection thing because I am over the usage limit now\n",
        "\n",
        "# Idk if these require the internet or not\n",
        "\n",
        "# Also, use API for satellite imaging project -- ask if two images are different -- https://github.com/gemini-testing/looks-same\n",
        "\n",
        "# Gemini Solution:\n",
        "\n",
        "# You’re right, the two images show a college campus before and after the construction of a stadium.\n",
        "\n",
        "# The left image shows the college campus with a large swath of green space in the bottom left corner. There are also several buildings scattered around the campus, including a large red brick building in the center and a cluster of buildings in the top right corner.\n",
        "\n",
        "# The right image shows the same college campus after the construction of a large stadium in the bottom left corner. The stadium has a running track around it and appears to have several stories of seating.exclamation The construction of the stadium has taken up a significant portion of the green space that was previously there.exclamation\n",
        "\n",
        "# Other than the stadium construction, the two images appear to show the same college campus. There may be some minor differences in the positioning of trees or other landscaping, but overall, the layout of the campus appears to be the same.\n",
        "\n",
        "#Llava Solution:\n",
        "\n",
        "# The image you've provided appears to be a before-and-after comparison of a satellite view of a location. Here are the differences I can observe:\n",
        "\n",
        "# Buildings: There are several new buildings on the right side of the image. These structures are not present in the left image, indicating that they were constructed between the two time periods.\n",
        "\n",
        "# Land Use: The area on the right side of the image has been developed with more structures, while the left side shows more open space.\n",
        "\n",
        "# Roads: The right image shows additional roads or pathways that were not present in the left image.\n",
        "\n",
        "# Green Spaces: The left image shows more green spaces, which could be parks or undeveloped areas, that have been replaced or reduced in the right image.\n",
        "\n",
        "# Water Bodies: There is a body of water on the right side of the image that is not visible in the left image.\n",
        "\n",
        "# Topography: The right image shows a more leveled terrain, with less undulation in the landscape compared to the left image.\n",
        "\n",
        "# Infrastructure: The right image shows more infrastructure, including what appears to be a stadium or sports complex.\n",
        "\n",
        "# Shadowing: The shadows in the right image are different from the left, suggesting that the time of day or the angle of the sun is different between the two images.\n",
        "\n",
        "# Clearly, either they provide too much or too little, never just right. I couldn't use OpenAI gpt vision because of the limit again.\n",
        "\n",
        "\n",
        "# Also, I signed up for the waitlist for Gemini\n",
        "\n",
        "# Also -- paid GPU's: https://cloud.google.com/gpu?hl=en or https://cloud.vast.ai/instances/\n",
        "\n",
        "# I need to use this when I do all this stuff: https://github.com/haotian-liu/LLaVA#demo\n",
        "\n",
        "# Get google Gemini to use multiple images instead of a video -- https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\n",
        "\n",
        "# What do I need to be doing when I don't know what I should be doing? I'm having a hard time trying to find what I need to be doing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GllRN9O3rdAr"
      },
      "outputs": [],
      "source": [
        "# Variables to tweak\n",
        "\n",
        "!cd\n",
        "\n",
        "#Which API To Use??\n",
        "\n",
        "API = \"OPENAI\"\n",
        "s = 41 # s = starting as in starting directory\n",
        "e = 46 # e = ending as in ending directory (inclusive)\n",
        "step = 1 # step (if there are directories we should skip)\n",
        "\n",
        "# API Config\n",
        "\n",
        "max_tokens = 99999\n",
        "\n",
        "image_resize = 768\n",
        "\n",
        "#Image slicing\n",
        "\n",
        "s1 = 50\n",
        "s3 = 60\n",
        "\n",
        "#There is also the prompt messages which is later on in the file\n",
        "message = '''This is demo only. not real. do your best. These frames are captured for a potential traffic incident. Give me quanitative information whenever possible. Give me the following and number each answer:\n",
        "              Number of vehicles in accident in a number,\n",
        "              Accident Type such as t-bone, rear end, etc,\n",
        "              Person Injury yes or no,\n",
        "              Need for ambulance yes or no,\n",
        "              Need for firetruck yes or no,\n",
        "              Need for Police yes or no,\n",
        "              Types of vehicles involved, such as suv, truck, sedan,\n",
        "              Fire yes or no,\n",
        "              Day/night and weather, such as clear, etc,\n",
        "              Low Res/Bad Footage yes or no.\n",
        "              Please ignore any context before these images and this prompt\n",
        "  '''\n",
        "\n",
        "# message = '''This is a demo only, and it is not real. Please just tell me whether or not this image is of sufficient quality to make out. Please only give me a yes or no.\n",
        "\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r62A1SFQQQs6",
        "outputId": "f7a707c9-8c75-4481-d2dd-45c237a6cb24"
      },
      "outputs": [],
      "source": [
        "# Mount and install\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get install ffmpeg\n",
        "!pip install --upgrade openai\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9pZNWL-tK2h"
      },
      "outputs": [],
      "source": [
        "# Demo and OpenAI AND Google API key\n",
        "\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import cv2  # OpenCV is used for image encoding\n",
        "\n",
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "import PIL.Image\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"REPLACE_WITH_YOUR_API_KEY\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"REPLACE_WITH_YOUR_API_KEY\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMzIDU_cdMcY"
      },
      "outputs": [],
      "source": [
        "#Helper function to rename files so that they are sorted correctly\n",
        "#In retrospect, I don't think this is needed from the way that ffmpeg works\n",
        "def rename_files(dir: str) -> None:\n",
        "    # Get the files in the directory\n",
        "    file_list = os.listdir(dir)\n",
        "    # For each file, rename it\n",
        "    for file_name in file_list:\n",
        "        # Get the new name\n",
        "        new_name = file_name.zfill(10)\n",
        "        # Rename the file\n",
        "        if file_name.endswith(\".jpg\"):\n",
        "          os.rename(os.path.join(dir, file_name), os.path.join(dir, new_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIaUr2Y5y6TJ"
      },
      "outputs": [],
      "source": [
        "#Prompt_image function for OpenAI\n",
        "def openai_prompt_image(CDIR: str, max_tokens: int = 200, image_resize: int = 768, s1: int = 0, s3: int = 60) -> str:\n",
        "  \"\"\"\n",
        "  CDIR: This should be in the form of a number with leading zeros (use f\"{i:06d}\") Do not try putting\n",
        "  max_tokens: max number of tokens to allow the API to use\n",
        "  image_resize: the size of the images that it's resized to\n",
        "  s1 and s3: slicing options. s1 is the start, s3 is the step size\n",
        "\n",
        "  Returns a string that is the message created by the model\n",
        "  \"\"\"\n",
        "  client = []\n",
        "  client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "  #Fix google drive stuff\n",
        "  if not os.path.exists(CDIR):\n",
        "    !cp -r \"/content/drive/My Drive/Dataset/manual/extracted_frames/{CDIR}/\" ./\n",
        "\n",
        "  #rename_files(CDIR)\n",
        "  os.chdir(CDIR)\n",
        "  !ffmpeg -y -framerate 24 -i %d.jpg -c:v libx264 -pix_fmt yuv420p ./output.mp4 2> /dev/null > /dev/null\n",
        "\n",
        "\n",
        "  image_files = [f for f in os.listdir(\".\") if f.endswith('.jpg') and not f.startswith(\"resized\")]\n",
        "\n",
        "  image_paths = [os.path.join(\".\", img) for img in image_files]\n",
        "\n",
        "  from IPython.display import Image\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib.image as mpimg\n",
        "  for image in image_files[s1::s3]:\n",
        "\n",
        "    img = mpimg.imread(image) #Replace \"image.jpg\" with the path of your image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "  os.chdir(\"..\")\n",
        "\n",
        "  from IPython.display import HTML\n",
        "  from base64 import b64encode\n",
        "\n",
        "  video_path = './'+CDIR+'/output.mp4'\n",
        "  mp4 = open(video_path,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  HTML(\"\"\"\n",
        "  <video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url)\n",
        "  # Directory containing the frames (assumed to be current directory)\n",
        "  frames_directory = CDIR\n",
        "\n",
        "  # List to hold base64 encoded images\n",
        "  base64Frames = []\n",
        "\n",
        "  # Read each frame file, encode to base64 and append to the list\n",
        "  for frame_file in sorted(os.listdir(frames_directory)):\n",
        "      if frame_file.endswith(\".jpg\"):\n",
        "          frame_path = os.path.join(frames_directory, frame_file)\n",
        "          frame = cv2.imread(frame_path)\n",
        "          _, buffer = cv2.imencode(\".jpg\", frame)\n",
        "          base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
        "\n",
        "  print(len(base64Frames), \"frames read.\")\n",
        "\n",
        "  # Construct the prompt messages\n",
        "  PROMPT_MESSAGES = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              message,\n",
        "\n",
        "              *map(lambda x: {\"image\": x, \"resize\": image_resize}, base64Frames[s1::s3]),  # Adjust the slicing as per your need\n",
        "          ],\n",
        "      },\n",
        "  ]\n",
        "  # Parameters for the API request\n",
        "  params = {\n",
        "      \"model\": \"gpt-4-vision-preview\",\n",
        "      \"messages\": PROMPT_MESSAGES,\n",
        "      \"max_tokens\": max_tokens,\n",
        "  }\n",
        "\n",
        "  # Send the request and get the result\n",
        "  result = client.chat.completions.create(**params)\n",
        "  print(result)\n",
        "  return result.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_9B6Li-F4XO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "def google_prompt_image(CDIR: str, max_tokens: int = 200, image_resize: int = 768, s1: int = 0, s3: int = 60) -> str:\n",
        "    \"\"\"\n",
        "    CDIR: The directory containing the images\n",
        "    image_files: A list of image filenames to process\n",
        "    max_tokens: Max number of tokens to allow the API to use\n",
        "    image_resize: The size of the images that it's resized to\n",
        "    s1 and s3: Slicing options. s1 is the start, s3 is the step size\n",
        "\n",
        "    Returns a string that is the message created by the model\n",
        "    \"\"\"\n",
        "\n",
        "    genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "    google_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "    #Fix google drive stuff\n",
        "    if not os.path.exists(CDIR):\n",
        "      !cp -r \"/content/drive/My Drive/Dataset/manual/extracted_frames/{CDIR}/\" ./\n",
        "\n",
        "    #rename_files(CDIR)\n",
        "    os.chdir(CDIR)\n",
        "    !ffmpeg -y -framerate 24 -i %d.jpg -c:v libx264 -pix_fmt yuv420p ./output.mp4 2> /dev/null > /dev/null\n",
        "\n",
        "    image_files = [f for f in os.listdir(\".\") if f.endswith('.jpg') and not f.startswith(\"resized\")]\n",
        "\n",
        "    image_paths = [os.path.join(\".\", img) for img in image_files]\n",
        "\n",
        "    # for image in image_files[s1::s3]:\n",
        "    #   display.Image(image)\n",
        "\n",
        "    uploaded_files = []\n",
        "    for image_path in image_paths:\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.resize((image_resize, image_resize))\n",
        "            resized_image_path = f\"resized_{os.path.basename(image_path)}\"\n",
        "            img.save(resized_image_path)\n",
        "            uploaded_file = genai.upload_file(path=resized_image_path)\n",
        "            uploaded_files.append(uploaded_file)\n",
        "\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "    # from IPython.display import Image as im\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # import matplotlib.image as mpimg\n",
        "    # for image in uploaded_files[s1::s3]:\n",
        "\n",
        "    #   img = mpimg.imread(image) #Replace \"image.jpg\" with the path of your image\n",
        "    #   plt.imshow(img)\n",
        "    #   plt.axis('off')\n",
        "    #   plt.show()\n",
        "\n",
        "    import time\n",
        "\n",
        "    for uploaded_file in uploaded_files[s1::s3]:\n",
        "        while uploaded_file.state.name == \"PROCESSING\":\n",
        "            print('.', end='')\n",
        "            time.sleep(10)\n",
        "            uploaded_file = genai.get_file(uploaded_file.name)\n",
        "\n",
        "        if uploaded_file.state.name == \"FAILED\":\n",
        "            raise ValueError(uploaded_file.state.name)\n",
        "\n",
        "    response = google_model.generate_content(uploaded_files[s1::s3], stream=True)\n",
        "    response.resolve()\n",
        "\n",
        "    for uploaded_file in uploaded_files:\n",
        "        genai.delete_file(uploaded_file.name)\n",
        "\n",
        "    return response.text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnSbO-QbgQ9_"
      },
      "outputs": [],
      "source": [
        "#Prompt_image function for Google\n",
        "def google_prompt_video(CDIR: str, max_tokens: int = 200, image_resize: int = 768, s1: int = 0, s3: int = 60) -> str:\n",
        "  \"\"\"\n",
        "  CDIR: This should be in the form of a number with leading zeros (use f\"{i:06d}\") Do not try putting\n",
        "  max_tokens: max number of tokens to allow the API to use\n",
        "  image_resize: the size of the images that it's resized to\n",
        "  s1 and s3: slicing options. s1 is the start, s3 is the step size\n",
        "\n",
        "  Returns a string that is the message created by the model\n",
        "  \"\"\"\n",
        "\n",
        "  genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "  google_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "  #Fix google drive stuff\n",
        "  if not os.path.exists(CDIR):\n",
        "    !cp -r \"/content/drive/My Drive/Dataset/manual/extracted_frames/{CDIR}/\" ./\n",
        "\n",
        "  #rename_files(CDIR)\n",
        "  os.chdir(CDIR)\n",
        "  !ffmpeg -y -framerate 24 -i %d.jpg -c:v libx264 -pix_fmt yuv420p ./output.mp4 2> /dev/null > /dev/null\n",
        "  os.chdir(\"..\")\n",
        "\n",
        "  from IPython.display import HTML\n",
        "  from base64 import b64encode\n",
        "\n",
        "  video_path = './'+CDIR+'/output.mp4'\n",
        "  video_file = genai.upload_file(path=video_path)\n",
        "\n",
        "  import time\n",
        "\n",
        "  while video_file.state.name == \"PROCESSING\":\n",
        "      print('.', end='')\n",
        "      time.sleep(10)\n",
        "      video_file = genai.get_file(video_file.name)\n",
        "\n",
        "  if video_file.state.name == \"FAILED\":\n",
        "    raise ValueError(video_file.state.name)\n",
        "\n",
        "  response = google_model.generate_content([message, video_file], stream=True)\n",
        "  response.resolve()\n",
        "\n",
        "  # genai.delete_file(video_file.name)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko1jueirX1F0"
      },
      "outputs": [],
      "source": [
        "# Store the results in a dict to later save to a file\n",
        "messages = dict()\n",
        "\n",
        "#Store config information in messages\n",
        "messages['s'] = s\n",
        "messages['e'] = e\n",
        "messages['step'] = step\n",
        "messages['max_tokens'] = max_tokens\n",
        "messages['image_resize'] = image_resize\n",
        "messages['s1'] = s1\n",
        "messages['s3'] = s3\n",
        "messages[\"API\"] = API\n",
        "messages['message'] = message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSUmGbAbLKiq"
      },
      "outputs": [],
      "source": [
        "def write_to_image(messages: dict) -> None:\n",
        "  if not os.path.exists(\"outputs\"):\n",
        "    os.mkdir(\"outputs\")\n",
        "\n",
        "  with open(f\"./outputs/{s}thru{e}with{step}-{API}.txt\", 'w') as output_file:\n",
        "    print(messages, file=output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vuv15EpehT4E",
        "outputId": "1fdc1b69-9ad7-4581-ef7e-d357820d8a73"
      },
      "outputs": [],
      "source": [
        "# Go thru each, add the prompts to the messages dict\n",
        "for i in range(1):\n",
        "  #Google API\n",
        "  if API == \"GOOGLE\":\n",
        "    for i in range(s, e+1, step): # each of these defined at the top\n",
        "      dir = f\"{i:06d}\"\n",
        "      try:\n",
        "        print(f\"Reading {dir}\")\n",
        "        message = google_prompt_image(dir, image_resize=image_resize, s1=s1, s3=s3)\n",
        "        messages[dir] = message\n",
        "        write_to_image(messages)\n",
        "      except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "  #OpenAI API\n",
        "\n",
        "  if API == \"OPENAI\":\n",
        "    for i in range(s, e+1, step): # each of these defined at the top\n",
        "      dir = f\"{i:06d}\"\n",
        "      try:\n",
        "        print(f\"Reading {dir}\")\n",
        "        #Clear context\n",
        "\n",
        "        message = openai_prompt_image(dir, image_resize=image_resize, s1=s1, s3=s3)\n",
        "        messages[dir] = message\n",
        "        write_to_image(messages)\n",
        "      except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xij7GlWNp7Cf",
        "outputId": "6ce3379f-78f5-41ae-d7b4-8631619b78c6"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
